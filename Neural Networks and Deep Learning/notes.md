# 神经网络和深度学习

## Logistic回归

Logistic 回归是一个用于二分分类的算法。Logistic 回归可以看作是一个非常小的神经网络。Sigmoid 函数：

$$
s=\sigma\left(w^{T} x+b\right)=\sigma(z)=\frac{1}{1+e^{-z}}
$$

## 损失函数

**损失函数**（loss function）用于衡量预测结果与真实值之间的误差。

最简单的损失函数定义方式为平方差损失：

$$
L(\hat{y}, y)=\frac{1}{2}(\hat{y}-y)^{2}
$$

但 Logistic 回归中并不倾向于使用这样的损失函数，因为优化问题会变成非凸的，最后会得到很多个局部最优解，梯度下降法可能找不到全局最优值。一般使用：

$$
L(\hat{y}, y)=-(y \log \hat{y})-(1-y) \log (1-\hat{y})
$$

损失函数是在单个训练样本中定义的，它衡量了在**单个**训练样本上的表现。而**代价函数**（cost function）衡量的是在全体训练样本上的表现，即衡量参数 w 和 b 的效果。

$$
J(w, b)=\frac{1}{m} \sum_{i=1}^{m} L\left(\hat{y}^{(i)}, y^{(i)}\right)
$$

## 梯度下降

函数的**梯度**（gradient）指出了函数的最陡增长方向。

$$
w:=w-\alpha \frac{d J(w, b)}{d w}
$$

## Numpy使用技巧

转置对秩为 1 的数组无效。因此，应该避免使用秩为 1 的数组，用 `n * 1` 的矩阵代替。例如，用 `np.random.randn(5,1)`代替 `np.random.randn(5)`。如果得到了一个秩为 1 的数组，可以使用 `reshape`进行转换。

## 激活函数

- **tanh 函数**（the hyperbolic tangent function，双曲正切函数）：

$$
a=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
$$

效果几乎总比 sigmoid 函数好（除了**二元分类的输出层**，因为我们希望输出的结果介于 0 到 1 之间），因为函数输出介于 -1 和 1 之间，激活函数的平均值就更接近 0，有类似数据中心化的效果。然而，tanh 函数存在和 sigmoid 函数一样的缺点：当 z 无穷大（或无穷小）时梯度趋近于 0，使得梯度算法的速度大大减缓。    

- **ReLU 函数**（the rectified linear unit，修正线性单元）：

$$
a=\max (0, z)
$$

当 z > 0 时，梯度始终为 1，从而提高神经网络基于梯度算法的运算速度，收敛速度远大于 sigmoid 和 tanh。然而当 z < 0 时，梯度一直为 0，但实际运用中该缺陷的影响不是很大。

- **Leaky ReLU**（带泄漏的 ReLU）：

$$
a=\max (0.01z, z)
$$

Leaky ReLU 保证在 z < 0 的时候，梯度仍然不为 0。理论上来说，Leaky ReLU 有 ReLU 的所有优点，但在实际操作中没有证明总是好于 ReLU，因此不常用。

## 使用非线性激活函数的原因

使用线性激活函数和不使用激活函数、直接使用 Logistic 回归没有区别，那么无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，就成了最原始的感知器了。

## 随机初始化

如果在初始时将两个隐藏神经元的参数设置为相同的大小，那么两个隐藏神经元对输出单元的影响也是相同的，通过反向梯度下降去进行计算的时候，会得到同样的梯度大小，所以在经过多次迭代后，两个隐藏层单位仍然是对称的。无论设置多少个隐藏单元，其最终的影响都是相同的，那么多个隐藏神经元就没有了意义。

在初始化的时候，W 参数要进行随机初始化，不可以设置为 0。而 b 因为不存在对称性的问题，可以设置为 0。以 2 个输入，2 个隐藏神经元为例：

```python
W = np.random.rand(2,2)* 0.01
b = np.zeros((2,1))
```

这里将 W 的值乘以 0.01 的原因是为了使得权重 W 初始化为较小的值。这是因为使用 sigmoid 函数或者 tanh 函数作为激活函数时，W 比较小，则 Z=WX+b 所得的值趋近于 0，梯度较大，能够提高算法的更新速度。而如果 W 设置的太大的话，得到的梯度较小，训练过程因此会变得很慢。用 ReLU 和 Leaky ReLU 作为激活函数时不存在这种问题，因为在大于 0 的时候，梯度均为 1。

## 前向和反向传播

神经网络的计算过程中，通常有一个正向过程（forward pass）或者叫**正向传播步骤（forward propagation step）**，接着会有一个反向过程（backward pass）或者叫**反向传播步骤（backward propagation step）**。

### 前向传播

输入：$a^{[l-1]}$

输出：$a^{[l]}, \text { cache }\left(z^{[l]}\right)$

公式：

$$
\begin{gathered}
Z^{[l]}=W^{[l]} \cdot a^{[l-1]}+b^{[l]} \\
a^{[l]}=g^{[l]}\left(Z^{[l]}\right)
\end{gathered}
$$

### 后向传播

输入：$d a^{[l]}$

输出：$d a^{[l-1]}, \quad d W^{[l]}, d b^{[l]}$

公式：

$$
\begin{gathered}
d Z^{[l]}=d a^{[l]} * g^{[l]}\left(Z^{[l]}\right) \\
d W^{[l]}=d Z^{[l]} \cdot a^{[l-1]} \\
d b^{[l]}=d Z^{[l]} \\
d a^{[l-1]}=W^{[l] T} \cdot d Z^{[l]}
\end{gathered}
$$

### 矩阵维度

$$
\begin{gathered}
W^{[l]}:\left(n^{[l]}, n^{[l-1]}\right) \\
b^{[l]}:\left(n^{[l]}, 1\right) \\
d W^{[l]}:\left(n^{[l]}, n^{[l-1]}\right) \\
d b^{[l]}:\left(n^{[l]}, 1\right)
\end{gathered}
$$

对于 Z、a，向量化之前有：

$$
Z^{[l]}, a^{[l]}:\left(n^{[l]}, 1\right)
$$

而在向量化之后，则有：

$$
Z^{[l]}, A^{[l]}:\left(n^{[l]}, m\right)
$$

在计算反向传播时，dZ、dA 的维度和 Z、A 是一样的。

## 使用深层表示的原因

对于人脸识别，神经网络的第一层从原始图片中提取人脸的轮廓和边缘，每个神经元学习到不同边缘的信息；网络的第二层将第一层学得的边缘信息组合起来，形成人脸的一些局部的特征，例如眼睛、嘴巴等；后面的几层逐步将上一层的特征组合起来，形成人脸的模样。随着神经网络层数的增加，特征也从原来的边缘逐步扩展为人脸的整体，由整体到局部，由简单到复杂。层数越多，那么模型学习的效果也就越精确。

同样的，对于语音识别，第一层神经网络可以学习到语言发音的一些音调，后面更深层次的网络可以检测到基本的音素，再到单词信息，逐渐加深可以学到短语、句子。通过例子可以看到，随着神经网络的深度加深，模型能学习到更加复杂的问题，功能也更加强大。

## 参数和超参数

**参数**是我们在过程中想要模型学习到的信息（**模型自己能计算出来的**）；而**超参数**（hyper parameters）为控制参数输出值的一些网络信息（**需要人经验判断**）。典型的超参数有：

- 学习速率
- 迭代次数
- 隐藏层的层数
- 每一层的神经元个数
- 激活函数的选择

当开发新应用时，预先很难准确知道超参数的最优值应该是什么。因此，通常需要尝试很多不同的值。应用深度学习领域是一个很大程度基于经验的过程。
