# 卷积神经网络

## 计算机视觉

一般的计算机视觉问题包括以下几类：

- 图片分类（Image Classification）
- 目标检测（Object detection）
- 神经风格转换（Neural Style Transfer）

应用计算机视觉时要面临的一个挑战是数据的输入可能会非常大。例如一张 1000x1000x3 的图片，神经网络输入层的维度将高达三百万，使得网络权重 $W$ 非常庞大。这样会造成两个后果：

1. 神经网络结构复杂，数据量相对较少，容易出现过拟合；
2. 所需内存和计算量巨大。

因此，一般的神经网络很难处理蕴含着大量数据的图像。解决这一问题的方法就是使用**卷积神经网络（Convolutional Neural Network, CNN）**。

## 卷积运算

神经网络由浅层到深层，分别可以检测出图片的边缘特征、局部特征，到最后面的一层就可以根据前面检测的特征来识别整体面部轮廓。这些工作都是依托卷积神经网络来实现的。

### 边缘检测

图片最常做的边缘检测有两类：**垂直边缘（Vertical Edges）检测**和**水平边缘（Horizontal Edges）检测**。

<img src="./src/Different-edges.png" title="" alt="Different-edges" width="442">

图片的边缘检测可以通过与相应滤波器进行卷积来实现。以垂直边缘检测为例，原始图片尺寸为 6x6，中间的矩阵被称作**滤波器（filter）**，尺寸为 3x3，卷积后得到的图片尺寸为 4x4，得到结果如下：

<img title="" src="./src/Vertical-Edge-Detection.jpg" alt="Vertical-Edge-Detection" width="327">

可以看到，卷积运算的求解过程是从左到右，由上到下，每次在原始图片矩阵中取与滤波器同等大小的一部分，每一部分中的值与滤波器中的值对应相乘后求和，将结果组成一个矩阵。下图对应一个垂直边缘检测的例子：

<img title="" src="./src/Convolutional-operation-example.jpg" alt="Convolutional-operation-example" width="394">

如果将最右边的矩阵当作图像，那么中间一段亮一些的区域对应最左边的图像中间的垂直边缘。图中的`*`表示卷积运算符号。在计算机中这个符号表示一般的乘法，而在不同的深度学习框架中，卷积操作的 API 定义可能不同：

- 在 Python 中，卷积用`conv_forward()`表示；
- 在 Tensorflow 中，卷积用`tf.nn.conv2d()`表示；
- 在 keras 中，卷积用`Conv2D()`表示。

### 更多边缘检测的例子

如果将灰度图左右的颜色进行翻转，再与之前的滤波器进行卷积，得到的结果也有区别。实际应用中，这反映了由明变暗和由暗变明的两种渐变方式。可以对输出图片取绝对值操作，以得到同样的结果。

<img title="" src="./src/Another-Convolutional-operation-example.jpg" alt="Another-Convolutional-operation-example" width="386">

垂直边缘检测和水平边缘检测的滤波器如下所示：

<img title="" src="./src/Vertical-and-Horizontal-Filter.png" alt="Convolutional-operation" width="244">

其他常用的滤波器还有 Sobel 滤波器和 Scharr 滤波器。它们增加了中间行的权重，以提高结果的稳健性。

<img src="./src/Sobel-Filter-and-Scharr-Filter.png" title="" alt="Sobel-Filter-and-Scharr-Filter" width="245">

滤波器中的值还可以设置为**参数**，通过模型训练来得到。这样，神经网络使用反向传播算法可以学习到一些低级特征，从而实现对图片所有边缘特征的检测，而不仅限于垂直边缘和水平边缘。

## 填充

假设输入图片的大小为 $n×n$，而滤波器的大小为 $f×f$，则卷积后的输出图片大小为 $(n−f+1)×(n−f+1)$。这样就有两个问题：

- 每次卷积运算后，输出图片的尺寸缩小；
- 原始图片的角落、边缘区像素点在输出中采用较少，输出图片丢失边缘位置的信息。

为了解决这些问题，可以在进行卷积操作前，对原始图片在边界上进行**填充（Padding）**，以增加矩阵的大小。通常将 0 作为填充值。

<img title="" src="./src/Padding.jpg" alt="Padding" width="393">

设每个方向扩展像素点数量为 $p$，则填充后原始图片的大小为 $(n+2p)×(n+2p)$，滤波器大小保持$ f×f$不变，则输出图片大小为 $(n+2p−f+1)×(n+2p−f+1)$。因此，在进行卷积运算时，我们有两种选择：

- **Valid 卷积**：不填充，直接卷积。结果大小为 $(n−f+1)×(n−f+1)$；
- **Same 卷积**：进行填充，并使得卷积后结果大小与输入一致，这样 $p=\frac{f−1}{2}$。

在计算机视觉领域，$f$通常为奇数。原因包括 Same 卷积中 $p=\frac{f−1}{2}$能得到自然数结果，并且滤波器有一个便于表示其所在位置的中心点。

## 卷积步长

卷积过程中，有时需要通过填充来避免信息损失，有时也需要通过设置步长（Stride）来压缩一部分信息。步长表示滤波器在原始图片的水平方向和垂直方向上每次移动的距离。之前，步长被默认为 1。而如果我们设置步长为 2，则卷积过程如下图所示：

<img src="./src/Stride.jpg" title="" alt="Stride" width="442">

设步长为 $s$，填充长度为 $p$，输入图片大小为 $n×n$，滤波器大小为 $f×f$，则卷积后图片的尺寸为：$⌊\frac{n+2p−f}{s}+1⌋×⌊\frac{n+2p−f}{s}+1⌋$。注意公式中有一个向下取整的符号，用于处理商不为整数的情况。向下取整反映着当取原始矩阵的图示蓝框完全包括在图像内部时，才对它进行运算。

目前为止我们学习的“卷积”实际上被称为**互相关（cross-correlation）**，而非数学意义上的卷积。真正的卷积操作在做元素乘积求和之前，要将滤波器沿水平和垂直轴翻转。因为这种翻转对一般为水平或垂直对称的滤波器影响不大，按照机器学习的惯例，我们通常不进行翻转操作，在简化代码的同时使神经网络能够正常工作。

## 高维卷积

如果我们想要对三通道的 RGB 图片进行卷积运算，那么其对应的滤波器组也同样是三通道的。过程是将每个单通道（R，G，B）与对应的滤波器进行卷积运算求和，然后再将三个通道的和相加，将 27 个乘积的**和**作为输出图片的一个像素值。

<img src="./src/Convolutions-on-RGB-image.png" title="" alt="Convolutions-on-RGB-image" width="390">

不同通道的滤波器可以不相同。例如只检测 R 通道的垂直边缘，G 通道和 B 通道不进行边缘检测，则 G 通道和 B 通道的滤波器全部置零。当输入有特定的高、宽和通道数时，滤波器可以有不同的高和宽，但通道数必须和输入一致。

如果想同时检测垂直和水平边缘，或者更多的边缘检测，可以增加更多的滤波器组。例如设置第一个滤波器组实现垂直边缘检测，第二个滤波器组实现水平边缘检测。设输入图片的尺寸为 $n×n×n_c$（$n_c$为通道数），滤波器尺寸为 $f×f×n_c$，则卷积后的输出图片尺寸为 $(n−f+1)×(n−f+1)×n^{′}_c$，其中$n^{′}_c$为滤波器组的个数。

<img src="./src/More-Filters.jpg" title="" alt="More-Filters" width="467">

## 单层卷积网络

<img src="./src/One-Layer-of-a-Convolutional-Network.jpg" title="" alt="One-Layer-of-a-Convolutional-Network" width="519">

与之前的卷积过程相比较，卷积神经网络的单层结构多了激活函数和偏移量；而与标准神经网络：

$Z[l]=W[l]A[l−1]+b$

$A[l]=g[l](Z[l])$

相比，滤波器的数值对应着权重 $W[l]$，卷积运算对应着 $W[l]$与 $A[l−1]$的乘积运算，所选的激活函数变为 ReLU。

对于一个 3x3x3 的滤波器，包括偏移量 b在内共有 28 个参数。不论输入的图片有多大，用这一个滤波器来提取特征时，参数始终都是 28 个，固定不变。即**选定滤波器组后，参数的数目与输入图片的尺寸无关**。因此，卷积神经网络的参数相较于标准神经网络来说要少得多。这是 CNN 的优点之一。

## 简单卷积网络示例

一个简单的 CNN 模型如下图所示：

![Simple-Convolutional-Network-Example](./src/Simple-Convolutional-Network-Example.jpg)

其中，$a^{[3]}$的维度为 7x7x40，将 1960 个特征平滑展开成 1960 个单元的一列，然后连接最后一级的输出层。输出层可以是一个神经元，即二元分类（logistic）；也可以是多个神经元，即多元分类（softmax）。最后得到预测输出 ŷ 。随着神经网络计算深度不断加深，图片的高度和宽度 $n^{[l]}_H$、$n^{[l]}_W$一般逐渐减小，而 $n^{[l]}_c$在增加。

一个典型的卷积神经网络通常包含有三种层：**卷积层（Convolution layer）**、**池化层（Pooling layer）**、**全连接层（Fully Connected layer）**。仅用卷积层也有可能构建出很好的神经网络，但大部分神经网络还是会添加池化层和全连接层，它们更容易设计。

## 池化层

**池化层**的作用是缩减模型的大小，提高计算速度，同时减小噪声，提高所提取特征的稳健性。

采用较多的一种池化过程叫做**最大池化（Max Pooling）**。将输入拆分成不同的区域，输出的每个元素都是对应区域中元素的最大值，如下图所示：

<img src="./src/Max-Pooling.png" title="" alt="Max-Pooling" width="401">

池化过程类似于卷积过程，上图所示的池化过程中相当于使用了一个大小 $f=2$的滤波器，且池化步长$ s=2$。卷积过程中的几个计算大小的公式也都适用于池化过程。如果有多个通道，那么就对每个通道分别执行计算过程。

对最大池化的一种直观解释是，元素值较大可能意味着池化过程之前的卷积过程提取到了某些特定的特征，池化过程中的最大化操作使得只要在一个区域内提取到某个特征，它都会保留在最大池化的输出中。但是，没有足够的证据证明这种直观解释的正确性。

另一种池化过程是**平均池化（Average Pooling）**，就是从取某个区域的最大值改为求这个区域的平均值：

<img src="./src/Average-Pooling.png" title="" alt="Average-Pooling" width="431">

池化过程的特点之一是，它有一组超参数，但是并**没有参数需要学习**。池化过程的超参数包括滤波器的大小 $f$、步长 $s$，以及选用最大池化还是平均池化。而填充$ p$则很少用到。

池化过程的输入维度为：

$n_H×n_W×n_c$

输出维度为：

$⌊\frac{n_H−f}{s}+1⌋×⌊\frac{n_W−f}{s}+1⌋×nc$

## 卷积神经网络示例

![CNN-Example](./src/CNN-Example.jpg)

在计算神经网络的层数时，通常只统计具有权重和参数的层，因此池化层通常和之前的卷积层共同计为一层。图中的 FC3 和 FC4 为全连接层，与标准的神经网络结构一致。整个神经网络各层的尺寸与参数如下表所示：

|                      | Activation Shape | Activation Size | #parameters             |
|:-------------------- | ---------------- | --------------- | ----------------------- |
| **Input**            | (32, 32, 3)      | 3072            | 0                       |
| **CONV1(f=5, s=1)**  | (28, 28, 8)      | 6272            | (5*5*3 + 1) * 8 = 608   |
| **POOL1**            | (14, 14, 8)      | 1568            | 0                       |
| **CONV2 (f=5, s=1)** | (10, 10, 16)     | 1600            | (5*5*8 + 1) * 16 = 3216 |
| **POOL2**            | (5, 5, 16)       | 400             | 0                       |
| **FC3**              | (120, 1)         | 120             | 400*120 + 120 = 48120   |
| **FC4**              | (84, 1)          | 84              | 120*84 + 84 = 10164     |
| **SoftMax**          | (10, 1)          | 10              | 84*10 + 10 = 850        |

## 使用卷积的原因

相比标准神经网络，对于大量的输入数据，卷积过程有效地减少了 CNN 的参数数量，原因有以下两点：

- **参数共享（Parameter sharing）**：特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。即在卷积过程中，不管输入有多大，一个特征探测器（滤波器）就能对整个输入的某一特征进行探测。
- **稀疏连接（Sparsity of connections）**：在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值。

池化过程则在卷积后很好地聚合了特征，通过降维来减少运算量。

由于 CNN 参数数量较小，所需的训练样本就相对较少，因此在一定程度上不容易发生过拟合现象。并且 CNN 比较擅长捕捉区域位置偏移。即进行物体检测时，不太受物体在图片中位置的影响，增加检测的准确性和系统的健壮性。

## 经典卷积网络

### LeNet-5

![LeNet-5](./src/LeNet-5.png)

特点：

- LeNet-5 针对灰度图像而训练，因此输入图片的通道数为 1。
- 该模型总共包含了约 6 万个参数，远少于标准神经网络所需。
- 典型的 LeNet-5 结构包含卷积层（CONV layer），池化层（POOL layer）和全连接层（FC layer），排列顺序一般为 CONV layer->POOL layer->CONV layer->POOL layer->FC layer->FC layer->OUTPUT layer。一个或多个卷积层后面跟着一个池化层的模式至今仍十分常用。
- 当 LeNet-5模型被提出时，其池化层使用的是平均池化，而且各层激活函数一般选用 Sigmoid 和 tanh。现在，我们可以根据需要，做出改进，使用最大池化并选用 ReLU 作为激活函数。

### AlexNet

<img src="./src/AlexNet.png" title="" alt="AlexNet" width="599">

特点：

- AlexNet 模型与 LeNet-5 模型类似，但是更复杂，包含约 6000 万个参数。另外，AlexNet 模型使用了 ReLU 函数。
- 当用于训练图像和数据集时，AlexNet 能够处理非常相似的基本构造模块，这些模块往往包含大量的隐藏单元或数据。

### VGG

<img src="./src/VGG.png" title="" alt="VGG" width="594">

特点：

- VGG 又称 VGG-16 网络，“16”指网络中包含 16 个卷积层和全连接层。
- 超参数较少，只需要专注于构建卷积层。
- 结构不复杂且规整，在每一组卷积层进行滤波器翻倍操作。
- VGG 需要训练的特征数量巨大，包含多达约 1.38 亿个参数。

## 残差网络

因为存在梯度消失和梯度爆炸问题，网络越深，就越难以训练成功。残差网络（Residual Network，简称为 ResNet）可以有效解决这个问题。

<img title="" src="./src/Residual-block.jpg" alt="Residual-block" width="359">

上图的结构被称为**残差块（Residual block）**。通过捷径（Short cut）可以将 $a[l]$添加到第二个 ReLU 过程中，直接建立$ a[l]$与$ a[l+2]$之间的隔层联系。表达式如下：

$z[l+1]=W[l+1]a[l]+b[l+1]$

$a[l+1]=g(z[l+1])$

$z[l+2]=W[l+2]a[l+1]+b[l+2]$

$a[l+2]=g(z[l+2]+a[l])$

构建一个残差网络就是将许多残差块堆积在一起，形成一个深度网络。

![Residual-Network](./src/Residual-Network.jpg)

为了便于区分，在 ResNets 的论文中，非残差网络被称为**普通网络（Plain Network）**。将它变为残差网络的方法是加上所有的跳远连接。

在理论上，随着网络深度的增加，性能应该越来越好。但实际上，对于一个普通网络，随着神经网络层数增加，训练错误会先减少，然后开始增多。但残差网络的训练效果显示，即使网络再深，其在训练集上的表现也会越来越好。

<img src="./src/ResNet-Training-Error.jpg" title="" alt="ResNet-Training-Error" width="503">

残差网络有助于解决梯度消失和梯度爆炸问题，使得在训练更深的网络的同时，又能保证良好的性能。

### 残差网络有效的原因

假设有一个大型神经网络，其输入为 $X$，输出为 $a[l]$。给这个神经网络额外增加两层，输出为$ a[l+2]$。将这两层看作一个具有跳远连接的残差块。为了方便说明，假设整个网络中都选用 ReLU 作为激活函数，因此输出的所有激活值都大于等于 0。

<img src="./src/Why-do-residual-networks-work.jpg" title="" alt="Why-do-residual-networks-work" width="548">

则有：

$a[l+2]=g(z[l+2]+a[l])=g(W[l+2]a[l+1]+b[l+2]+a[l])$

当发生梯度消失时，$W[l+2]≈0$，$b[l+2]≈0$，则有：

$a[l+2]=g(a[l])=ReLU(a[l])=a[l]$

因此，这两层额外的残差块不会降低网络性能。而如果没有发生梯度消失时，训练得到的非线性关系会使得表现效果进一步提高。

如果 $a[l]$与 $a[l+2]$的维度不同，需要引入矩阵 $W_s$与 $a[l]$相乘，使得二者的维度相匹配。参数矩阵 $W_s$既可以通过模型训练得到，也可以作为固定值，仅使 $a[l]$截断或者补零。

<img src="./src/ResNet-Paper.png" title="" alt="ResNet-Paper" width="600">

## 1x1 卷积

1x1 卷积指滤波器的尺寸为 1。当通道数为 1 时，1x1 卷积意味卷积操作等同于乘积操作。

<img src="./src/1x1-Conv-1.png" title="" alt="1x1-Conv-1" width="489">

而当通道数更多时，1x1 卷积的作用实际上类似全连接层的神经网络结构，从而降低（或升高，取决于滤波器组数）数据的维度。

池化能压缩数据的高度（$n_H$）及宽度（$n_W$），而 1×1 卷积能压缩数据的通道数（$n_C$）。在如下图所示的例子中，用 32 个大小为 1×1×192 的滤波器进行卷积，就能使原先数据包含的 192 个通道压缩为 32 个。

<img title="" src="./src/1x1-Conv-2.png" alt="1x1-Conv-2" width="492">

## Inception 网络

在之前的卷积网络中，我们只能选择单一尺寸和类型的滤波器。而 **Inception 网络的作用**即是代替人工来确定卷积层中的滤波器尺寸与类型，或者确定是否需要创建卷积层或池化层。

<img title="" src="./src/Motivation-for-inception-network.jpg" alt="Motivation-for-inception-network" width="509">

如图，Inception 网络选用不同尺寸的滤波器进行 Same 卷积，并将卷积和池化得到的输出组合拼接起来，最终让网络自己去学习需要的参数和采用的滤波器组合。

### 计算成本问题

在提升性能的同时，Inception 网络有着较大的计算成本。下图是一个例子：

<img title="" src="./src/The-problem-of-computational-cost.png" alt="The-problem-of-computational-cost" width="386">

图中有 32 个滤波器，每个滤波器的大小为 5x5x192。输出大小为 28x28x32，所以需要计算 28x28x32 个数字，对于每个数，都要执行 5x5x192 次乘法运算。加法运算次数与乘法运算次数近似相等。因此，可以看作这一层的计算量为 28x28x32x5x5x192 = 1.2亿。

为了解决计算量大的问题，可以引入 1x1 卷积来减少其计算量。

<img src="./src/Using-1x1-convolution.png" title="" alt="Using-1x1-convolution" width="457">

我们使用 1x1 卷积把输入数据从 192 个通道减少到 16 个通道，然后对这个较小层运行 5x5 卷积，得到最终输出。这个 1x1 的卷积层通常被称作**瓶颈层（Bottleneck layer）**。改进后的计算量为 28x28x192x16 + 28x28x32x5x5x15 = 1.24 千万，减少了约 90%。只要合理构建瓶颈层，就可以既显著缩小计算规模，又不会降低网络性能。

### 完整的 Inception 网络

<img src="./src/Inception-module.jpg" title="" alt="Inception-module" width="413">

上图是引入 1x1 卷积后的 Inception 模块。值得注意的是，为了将所有的输出组合起来，红色的池化层使用 Same 类型的填充（padding）来池化使得输出的宽高不变，通道数也不变。多个 Inception 模块组成一个完整的 Inception 网络（被称为 GoogLeNet，以向 LeNet 致敬），如下图所示：

<img src="./src/Inception-network.jpg" title="" alt="Inception-network" width="592">

注意黑色椭圆圈出的隐藏层，这些分支都是 Softmax 的输出层，可以用来参与特征的计算及结果预测，起到调整并防止发生过拟合的效果。

## MobileNet

主要用于低计算能力的处理器，比如手机。工作原理：深度可分离卷积。

<img src="./src/depthwise-convolution-1.png#pic_center" title="" alt="" width="503">

深度可分离卷积分为两个步骤：深度卷积（Depthwise）和逐点卷积（Pointwise）。

<img src="./src/depthwise-convolution-2.png#pic_center" title="" alt="在这里插入图片描述" width="503">

## 迁移学习

计算机视觉是一个经常用到迁移学习的领域。在搭建计算机视觉的应用时，相比于从头训练权重，下载别人已经训练好的网络结构的权重，用其做**预训练**，然后转换到自己感兴趣的任务上，有助于加速开发。

对于已训练好的卷积神经网络，可以将所有层都看作是**冻结的**，只需要训练与你的 Softmax 层有关的参数即可。而冻结的层由于不需要改变和训练，可以看作一个固定函数。可以将这个固定函数存入硬盘，以便后续使用，而不必每次再使用训练集进行训练了。

上述的做法适用于你只有一个较小的数据集。如果你有一个更大的数据集，应该冻结更少的层，然后训练后面的层。越多的数据意味着冻结越少的层，训练更多的层。如果有一个极大的数据集，你可以将开源的网络和它的权重整个当作初始化（代替随机初始化），然后训练整个网络。

## 数据扩增

计算机视觉领域的应用都需要大量的数据。数据不够时，数据扩增（Data Augmentation）就有帮助。常用的数据扩增包括镜像翻转、随机裁剪、色彩转换。

## 计算机视觉现状

通常，学习算法有两种知识来源：

- 被标记的数据
- 手工工程

手工工程（Hand-engineering，又称 hacks）指精心设计的特性、网络体系结构或是系统的其他组件。手工工程是一项非常重要也比较困难的工作。在数据量不多的情况下，手工工程是获得良好表现的最佳方式。正因为数据量不能满足需要，历史上计算机视觉领域更多地依赖于手工工程。近几年数据量急剧增加，因此手工工程量大幅减少。

另外，在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：

- 集成（Ensembling）：独立地训练几个神经网络，并平均输出它们的输出
- Multi-crop at test time：将数据扩增应用到测试集，对结果进行平均

但是由于这些方法计算和内存成本较大，一般不适用于构建实际的生产项目。

## 目标检测

目标检测是计算机视觉领域中一个新兴的应用方向，其任务是对输入图像进行分类的同时，检测图像中是否包含某些目标，并对他们准确定位并标识。

## 目标定位

定位分类问题不仅要求判断出图片中物体的种类，还要在图片中标记出它的具体位置，用边框（Bounding Box）把物体圈起来。一般来说，定位分类问题通常只有一个较大的对象位于图片中间位置；而在目标检测问题中，图片可以含有多个对象，甚至单张图片中会有多个不同分类的对象。

为了定位图片中汽车的位置，可以让神经网络多输出 4 个数字，标记为 $bx$、$by$、$bh$、$bw$。将图片左上角标记为 (0, 0)，右下角标记为 (1, 1)，则有：

- 红色方框的中心点：($bx$，$by$)
- 边界框的高度：$bh$
- 边界框的宽度：$bw$

因此，训练集不仅包含对象分类标签，还包含表示边界框的四个数字。定义目标标签 Y 如下：

$\left[\begin{array}{l}P_{c} \\ b_{x} \\ b_{y} \\ b_{h} \\ b_{w} \\ c_{1} \\ c_{2} \\ c_{3}\end{array}\right]$

则有：

$P_{c}=1, Y=\left[\begin{array}{c}1 \\ b_{x} \\ b_{y} \\ b_{h} \\ b_{w} \\ c_{1} \\ c_{2} \\ c_{3}\end{array}\right]$

其中，$c_n$表示存在第 $n$个种类的概率；如果 $P_c=0$，表示没有检测到目标，则输出标签后面的 7 个参数都是无效的，可以忽略（用 `?` 来表示）。

$P_{c}=0, Y=\left[\begin{array}{l}0 \\ ? \\ ? \\ ? \\ ? \\ ? \\ ? \\ ?\end{array}\right]$

损失函数可以表示为 $L(ŷ ,y)$，如果使用平方误差形式，对于不同的 $P_c$有不同的损失函数（注意下标 $i$指标签的第 $i$个值）：

1. $P_c=1$，即$y_1=1$：
   
   $L(ŷ ,y)=(ŷ_1−y_1)^2+(ŷ_2−y_2)^2+⋯+(ŷ_8−y_8)^2$

2. $P_c=0$，即$y_1=0$：
   
   $L(ŷ ,y)=(ŷ_1−y_1)^2$

除了使用平方误差，也可以使用逻辑回归损失函数，类标签 $c_1,c_2,c_3$ 也可以通过 softmax 输出。相比较而言，平方误差已经能够取得比较好的效果。

## 特征点检测

神经网络可以像标识目标的中心点位置那样，通过输出图片上的特征点，来实现对目标特征的识别。在标签中，这些特征点以多个二维坐标的形式表示。

通过检测人脸特征点可以进行情绪分类与判断，或者应用于 AR 领域等等。也可以透过检测姿态特征点来进行人体姿态检测。

## 目标检测

想要实现目标检测，可以采用基于滑动窗口的目标检测（Sliding Windows Detection）算法。该算法的步骤如下：

1. 训练集上搜集相应的各种目标图片和非目标图片，样本图片要求尺寸较小，相应目标居于图片中心位置并基本占据整张图片。
2. 使用训练集构建 CNN 模型，使得模型有较高的识别率。
3. 选择大小适宜的窗口与合适的固定步幅，对测试图片进行从左到右、从上倒下的滑动遍历。每个窗口区域使用已经训练好的 CNN 模型进行识别判断。
4. 可以选择更大的窗口，然后重复第三步的操作。

<img src="./src/Sliding-windows-detection.png" title="" alt="Sliding-windows-detection" width="500">

滑动窗口目标检测的**优点**是原理简单，且不需要人为选定目标区域；**缺点**是需要人为直观设定滑动窗口的大小和步幅。滑动窗口过小或过大，步幅过大均会降低目标检测的正确率。另外，每次滑动都要进行一次 CNN 网络计算，如果滑动窗口和步幅较小，计算成本往往很大。滑动窗口目标检测算法虽然简单，但是性能不佳，效率较低。

## 基于卷积的滑动窗口实现

相比从较大图片多次截取，在卷积层上应用滑动窗口目标检测算法可以提高运行速度。所要做的仅是将全连接层换成卷积层，即使用与上一层尺寸一致的滤波器进行卷积运算。

<img src="./src/Convolution-implementation-of-sliding-windows.png" title="" alt="Convolution-implementation-of-sliding-windows" width="591">

如图，对于 16x16x3 的图片，步长为 2，CNN 网络得到的输出层为 2x2x4。其中，2x2 表示共有 4 个窗口结果。对于更复杂的 28x28x3 的图片，得到的输出层为 8x8x4，共 64 个窗口结果。最大池化层的宽高和步长相等。

运行速度提高的原理：在滑动窗口的过程中，需要重复进行 CNN 正向计算。因此，不需要将输入图片分割成多个子集，分别执行向前传播，而是将它们作为一张图片输入给卷积网络进行一次 CNN 正向计算。这样，公共区域的计算可以共享，以降低运算成本。

## 边框预测

在上述算法中，边框的位置可能无法完美覆盖目标，或者大小不合适，或者最准确的边框并非正方形，而是长方形。

**YOLO（You Only Look Once）算法**可以用于得到更精确的边框。YOLO 算法将原始图片划分为 n×n 网格，并将目标定位一节中提到的图像分类和目标定位算法，逐一应用在每个网格中，每个网格都有标签如：

$\left[\begin{array}{l}P_{c} \\ b_{x} \\ b_{y} \\ b_{h} \\ b_{w} \\ c_{1} \\ c_{2} \\ c_{3}\end{array}\right]$

若某个目标的中点落在某个网格，则该网格负责检测该对象。

<img src="./src/Bounding-Box-Predictions.png" title="" alt="Bounding-Box-Predictions" width="608">

如上面的示例中，如果将输入的图片划分为 3×3 的网格、需要检测的目标有 3 类，则每一网格部分图片的标签会是一个 8 维的列矩阵，最终输出的就是大小为 3×3×8 的结果。要得到这个结果，就要训练一个输入大小为 100×100×3，输出大小为 3×3×8 的 CNN。在实践中，可能使用更为精细的 19×19 网格，则两个目标的中点在同一个网格的概率更小。

YOLO 算法的优点：

1. 和图像分类和目标定位算法类似，显式输出边框坐标和大小，不会受到滑窗分类器的步长大小限制。
2. 仍然只进行一次 CNN 正向计算，效率很高，甚至可以达到实时识别。

YOLO 算法设 $bx$、$by$、$bh$、$bw $的值是相对于网格长的比例。则 $bx$、$by$ 在 0 到 1 之间，而 $bh$、$bw$ 可以大于 1。当然，也有其他参数化的形式，且效果可能更好。这里只是给出一个通用的表示方法。

## 交互比

交互比（IoU, Intersection Over Union）函数用于评价对象检测算法，它计算预测边框和实际边框交集（I）与并集（U）之比：

$I o U=\frac{I}{U}$

$IoU$ 的值在 0～1 之间，且越接近 1 表示目标的定位越准确。$IoU $大于等于 0.5 时，一般可以认为预测边框是正确的，当然也可以更加严格地要求一个更高的阈值。

## 非极大值抑制

YOLO 算法中，可能有很多网格检测到同一目标。非极大值抑制（Non-max Suppression）会通过清理检测结果，找到每个目标中点所位于的网格，确保算法对每个目标只检测一次。

进行非极大值抑制的步骤如下：

1. 将包含目标中心坐标的可信度 $P_c$ 小于阈值（例如 0.6）的网格丢弃；
2. 选取拥有最大 $P_c $的网格；
3. 分别计算该网格和其他所有网格的 $IoU$，将 $IoU $超过预设阈值的网格丢弃；
4. 重复第 2~3 步，直到不存在未处理的网格。

上述步骤适用于单类别目标检测。进行多个类别目标检测时，对于每个类别，应该单独做一次非极大值抑制。

## Anchor Boxes

到目前为止，我们讨论的情况都是一个网格只检测一个对象。如果要将算法运用在多目标检测上，需要用到 Anchor Boxes。一个网格的标签中将包含多个 Anchor Box，相当于存在多个用以标识不同目标的边框。

<img src="./src/Overlapping-objects.png" title="" alt="Overlapping-objects" width="553">

在上图示例中，我们希望同时检测人和汽车。因此，每个网格的的标签中含有两个 Anchor Box。输出的标签结果大小从 3×3×8 变为 3×3×16。若两个 $P_c$ 都大于预设阈值，则说明检测到了两个目标。

在单目标检测中，图像中的目标被分配给了包含该目标中点的那个网格；引入 Anchor Box 进行多目标检测时，图像中的目标则被分配到了包含该目标中点的那个网格以及具有最高 IoU 值的该网格的 Anchor Box。

Anchor Boxes 也有局限性，对于同一网格有三个及以上目标，或者两个目标的 Anchor Box 高度重合的情况处理不好。

## R-CNN

前面介绍的滑动窗口目标检测算法对一些明显没有目标的区域也进行了扫描，这降低了算法的运行效率。为了解决这个问题，R-CNN（Region CNN，带区域的 CNN）**被提出。通过对输入图片运行**图像分割算法，在不同的色块上找出**候选区域（Region Proposal）**，就只需要在这些区域上运行分类器。

<img src="./src/R-CNN.png" title="" alt="R-CNN" width="623">

R-CNN 的缺点是运行速度很慢，所以有一系列后续研究工作改进，例如 Fast R-CNN（与基于卷积的滑动窗口实现相似，但得到候选区域的聚类步骤依然很慢）、Faster R-CNN（使用卷积对图片进行分割），不过大多数时候还是比 YOLO 算法慢。

## 基于 U-Net 的语义分割

语义分割不是仅仅给出单个类标签或者可能是指定边界框所需的类标签和坐标，在这种情况下，神经网络单元必须生成整个标签矩阵。

<img title="" src="./src/object-detection.png#pic_center" alt="在这里插入图片描述" width="477">

转置卷积是 U-Net 的关键，它使得小的输入变成大的输出。具体操作中重叠部分元素相加，而非覆盖。

<img title="" src="./src/transpose-convulution.png#pic_center" alt="在这里插入图片描述" width="486">

两种类型的信息是有用的。一个是它从前一层获得的高级、空间、高级上下文信息，缺少的是非常详细、细粒度的空间信息。跳跃连接允许神经网络获取这种非常高分辨率、低级别的特征信息，它可以捕获每个像素位置。这样一来，这一层既具有较低的分辨率、高层次、空间、高层次的上下文信息，以及低层次、更详细的纹理信息，以便决定某个像素是否是猫的一部分。

<img src="./src/semantic-segmentation.png#pic_center" title="" alt="在这里插入图片描述" width="510">

U-Net 的架构图如下。

<img src="./src/u-net.png#pic_center" title="" alt="在这里插入图片描述" width="511">

## 人脸识别

人脸验证（Face Verification）和人脸识别（Face Recognition）的区别：

- 人脸验证：一般指一个一对一问题，只需要验证输入的人脸图像是否与某个已知的身份信息对应；
- 人脸识别：一个更为复杂的一对多问题，需要验证输入的人脸图像是否与多个已知身份信息中的某一个匹配。

一般来说，由于需要匹配的身份信息更多导致错误率增加，人脸识别比人脸验证更难一些。

### One-Shot 学习

人脸识别所面临的一个挑战是要求系统只采集某人的一个面部样本，就能快速准确地识别出这个人，即只用一个训练样本来获得准确的预测结果。这被称为**One-Shot 学习**。

有一种方法是假设数据库中存有 $N $个人的身份信息，对于每张输入图像，用 Softmax 输出 $N+1 $种标签，分别对应每个人以及都不是。然而这种方法的实际效果很差，因为过小的训练集不足以训练出一个稳健的神经网络；并且如果有新的身份信息入库，需要重新训练神经网络，不够灵活。因此，我们通过学习一个 Similarity 函数来实现 One-Shot 学习过程。Similarity 函数定义了输入的两幅图像的差异度，其公式如下：

$Similarity=d(img_1,img_2)$

可以设置一个超参数$ τ$ 作为阈值，作为判断两幅图片是否为同一个人的依据。

### Siamese 网络

实现 Similarity 函数的一种方式是使用**Siamese 网络**，它是一种对两个不同输入运行相同的卷积网络，然后对它们的结果进行比较的神经网络。

<img src="./src/Siamese.png" title="" alt="Siamese" width="499">

如上图示例，将图片 $x(1)$、$x(2)$ 分别输入两个相同的卷积网络中，经过全连接层后不再进行 Softmax，而是得到特征向量 $f(x(1))$、$f(x(2))$。这时，Similarity 函数就被定义为两个特征向量之差的 L2 范数：

$d(x(1),x(2))=||f(x(1))−f(x(2))||^2_2

### Triplet 损失

**Triplet 损失函数**用于训练出合适的参数，以获得高质量的人脸图像编码。“Triplet”一词来源于训练这个神经网络需要大量包含 Anchor、Positive、Negative的图片组，其中 Anchor 和 Positive 需要是同一个人的人脸图像。

<img src="./src/Training-set-using-triplet-loss.png" title="" alt="Training-set-using-triplet-loss" width="484">

对于这三张图片，应该有：

$||f(A)−f(P)||^2_2+α≤||f(A)−f(N)||^2_2$

其中，$α$ 被称为**间隔（margin）**，用于确保 $f(\cdot)$ 不会总是输出零向量（或者一个恒定的值）。Triplet 损失函数的定义：

$L(A,P,N)=max(||f(A)−f(P)||^2_2−||f(A)−f(N)||^2_2+α,0)$

其中，因为 $||f(A)−f(P)||^2_2−||f(A)−f(N)||^2_2+α$ 的值需要小于等于 0，因此取它和 0 的更大值。对于大小为 m 的训练集，代价函数为：

$J=\sum_{i=1}^{m}L(A(i),P(i),N(i))$

通过梯度下降最小化代价函数。

在选择训练样本时，随机选择容易使 Anchor 和 Positive 极为接近，而 Anchor 和 Negative 相差较大，以致训练出来的模型容易抓不到关键的区别。因此，最好的做法是人为增加 Anchor 和 Positive 的区别，缩小 Anchor 和 Negative 的区别，促使模型去学习不同人脸之间的关键差异。

### 二分类结构

除了 Triplet 损失函数，二分类结构也可用于学习参数以解决人脸识别问题。其做法是输入一对图片，将两个 Siamese 网络产生的特征向量输入至同一个 Sigmoid 单元，输出 1 则表示是识别为同一人，输出 0 则表示识别为不同的人。

Sigmoid 单元对应的表达式为：

$\left.\hat{y}=\sigma\left(\sum_{k=1}^{K} w_{k} \mid f\left(x^{(i)}\right)_{k}-x^{(j)}\right)_{k} \mid+b\right)$

其中，wk 和 b 都是通过梯度下降算法迭代训练得到的参数。上述计算表达式也可以用另一种表达式代替：

$\hat{y}=\sigma\left(\sum_{k=1}^{K} w_{k} \frac{\left(f\left(x^{(i)}\right)_{k}-f\left(x^{(j)}\right)_{k}\right)^{2}}{f\left(x^{(i)}\right)_{k}+f\left(x^{(j)}\right)_{k}}+b\right)$

其中，$\frac{\left(f\left(x^{(i)}\right)_{k}-f\left(x^{(j)}\right)_{k}\right)^{2}}{f\left(x^{(i)}\right)_{k}+f\left(x^{(j)}\right)_{k}}$ 被称为 $\chi$方相似度。

无论是对于使用 Triplet 损失函数的网络，还是二分类结构，为了减少计算量，可以提前计算好编码输出$ f(x) $并保存。这样就不必存储原始图片，并且每次进行人脸识别时只需要计算测试图片的编码输出。

## 神经风格迁移

神经风格迁移（Neural style transfer）将参考风格图像的风格“迁移”到另外一张内容图像中，生成具有其特色的图像。

<img src="./src/Neural-style-transfer.png" title="" alt="Neural-style-transfer" width="491">

### 深度卷积网络在学什么？

想要理解如何实现神经风格转换，首先要理解在输入图像数据后，一个深度卷积网络从中都学到了些什么。我们借助可视化来做到这一点。

<img src="./src/Visualizing-deep-layers.png" title="" alt="Visualizing-deep-layers" width="610">

通过遍历所有的训练样本，找出使该层激活函数输出最大的 9 块图像区域。可以看出，浅层的隐藏层通常检测出的是原始图像的边缘、颜色、阴影等简单信息。随着层数的增加，隐藏单元能捕捉的区域更大，学习到的特征也由从边缘到纹理再到具体物体，变得更加复杂。

### 代价函数

神经风格迁移生成图片 G 的代价函数如下：

$J(G)=α⋅J_{content}(C,G)+β⋅J_{style}(S,G)$

其中，$α、β$ 是用于控制相似度比重的超参数。

神经风格迁移的算法步骤如下：

1. 随机生成图片 $G $的所有像素点；
2. 使用梯度下降算法使代价函数最小化，以不断修正 $G$ 的所有像素点。

#### 内容代价函数

上述代价函数包含内容代价部分和风格代价部分。先来讨论内容代价函数 $J_{content}(C,G)$，它表示内容图片 $C $和生成图片$ G $之间的相似度。$J_{content}(C,G)$ 的计算过程如下：

- 使用一个预训练好的 CNN（例如 VGG）；
- 选择一个隐藏层 $l $来计算内容代价。$l$ 太小则内容图片和生成图片像素级别相似，$l $太大则可能只有具体物体级别的相似。因此，$l$ 一般选一个中间层；
- 设 $a^{(C)[l]}$、$a^{(G)[l]}$ 为 $C$ 和 $G$ 在 $l $层的激活，则有：

$J_{content}(C,G)=\frac{1}{2}||(a^{(C)[l]}−a^{(G)[l]})||^2$

#### 风格代价函数

<img src="./src/Intuition-about-style-of-an-image.png" title="" alt="Intuition-about-style-of-an-image" width="238">

每个通道提取图片的特征不同，比如红色的通道提取的是图片的垂直纹理特征，黄色的通道提取的是图片的橙色背景特征。计算这两个通道的相关性，相关性的大小即表示原始图片既包含了垂直纹理也包含了该橙色背景的可能性大小。**通过 CNN，“风格”被定义为同一个隐藏层不同通道之间激活值的相关系数，因其反映了原始图片特征间的相互关系。**

对于风格图像 $S$，选定网络中的第$ l $层，则相关系数以一个 gram 矩阵的形式表示：

$G_{k k^{\prime}}^{[l](S)}=\sum_{i=1}^{n_{H}^{[l]}} \sum_{j=1}^{n_{W}^{[l]}} a_{i j k}^{[l](S)} a_{i j k}^{[l](S)}$

其中，$i $和 $j$ 为第$ l $层的高度和宽度；$k $和 $k^′ $为选定的通道，其范围为 $1 $到 $n^{[l]}_{C}$；$a_{i j k}^{[l](S)}$ 为激活。同理，对于生成图像 $G$，有：

$G_{k k^{\prime}}^{[l](S)}=\sum_{i=1}^{n_{H}^{[l]}} \sum_{j=1}^{n_{W}^{[l]}} a_{i j k}^{[l](G)} a_{i j k}^{[l](G)}$

因此，第 $l$ 层的风格代价函数为：

$J_{s t y l e}^{[l]}(S, G)=\frac{1}{\left(2 n_{H}^{[l]} n_{W}^{[l]} n_{C}^{[l]}\right)^{2}} \sum_{k} \sum_{k^{\prime}}\left(G_{k k^{\prime}}^{[l](S)}-G_{k k^{\prime}}^{[l](G)}\right)^{2}$

如果对各层都使用风格代价函数，效果会更好。因此有：

$J_{s t y l e}(S, G)=\sum_{l} \lambda^{[l]} J_{s t y l e}^{[l]}(S, G)$

其中，$\lambda$ 是用于设置不同层所占权重的超参数。
